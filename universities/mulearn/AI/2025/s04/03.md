---
country: "india"
university: "mulearn"
branch: "ai"
version: "2019"
semester: "4"
course_code: "cl-ai-gradientdescent"
course_title: "gradient-descent"
language: "english"
contributor: "@arnavam"

---

# Gradient Descent

## Task Objectives

* _Grasp the abstract concept_ of using optimization algorithms, like **Stochastic Gradient Descent (SGD)**, to find solutions to complex problems.
* _Develop an intuitive understanding_ of how a mathematical problem can be reframed as a "learning" task for an AI model.
* _Build a foundational knowledge_ of core neural network components, including activation functions (like **ReLU**) and loss metrics.
* _Cultivate professional skills_ in documenting and presenting a complete machine learning experiment in a shareable portfolio format.

---
## Course Outcomes

* **CO1:** Demonstrate a conceptual understanding of how **SGD** iteratively guides a model toward a solution by minimizing error.
* **CO2:** Frame a traditional algebraic equation as a supervised learning problem by generating a dataset of inputs and corresponding outputs.
* **CO3:** Implement a simple neural network architecture and justify the choice of a loss function (e.g., **Mean Squared Error**) for a regression task.
* **CO4:** Analyze the training process to show how a model learns to approximate a non-linear function, effectively "solving" the initial equation.

---

## Syllabus Modules

### Module 1: Theoretical Framework
* üìö **Internalize the core principles** of gradient-based optimization by studying the provided material from [fast.ai Lesson 3](https://course.fast.ai/Lessons/lesson3.html).
* **Reframe the problem**: In your notebook, define a cubic equation ($f(x) = ax^3 + bx^2 + cx + d$) and generate a synthetic dataset from it. This transforms an algebraic problem into a machine learning task where the goal is to learn the function's behavior. 

### Module 2: Practical Implementation and Training
* **Construct the "learner"**: Build a simple neural network with a **ReLU** activation function from scratch.
* **Define the "goal"**: Implement a **Mean Squared Error** loss function to measure the difference between your model's guesses and the actual function values.
* **Initiate the learning process**: Create a training loop that uses **SGD** to repeatedly:
    1.  Calculate the loss.
    2.  Compute gradients (the direction of steepest error increase).
    3.  Update the model's parameters in the opposite direction to minimize the error.
* **Observe and document** the model's convergence as the loss decreases over multiple epochs.

### Module 3: Documentation and Submission
* **Synthesize your findings** into a clean, well-commented Jupyter Notebook that tells the story of your experiment from start to finish.
* **Showcase your work** by uploading the completed notebook (`.ipynb` file) to a new GitHub repository.
* üì§ **Submit your project** by sharing your GitHub repository URL in the `‚Å†#ai` channel with the hashtag `#cl-ai-gradientdescent` to earn ‚≠ê **500 karma points**.

---
## References
* [fast.ai Lesson 3: "Foundations"](https://course.fast.ai/Lessons/lesson3.html)
